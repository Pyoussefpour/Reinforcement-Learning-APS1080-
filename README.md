# APS1080 â€“ Reinforcement Learning (Graduate Course)

This repository contains coursework and assignments for **APS1080: Reinforcement Learning**, a graduate-level course at the **University of Toronto**.

## ğŸ“˜ Course Overview

APS1080 provides a comprehensive introduction to Reinforcement Learning (RL), covering both classical methods and modern deep RL techniques. The course blends theoretical foundations with hands-on implementation in Python.

### Core Topics Covered:
- **RL Fundamentals**: Agent-environment setup, Markov Decision Processes (MDPs), rewards, value functions
- **Model-Based Methods**: Dynamic Programming for planning and policy improvement
- **Model-Free Methods**: Monte Carlo, TD(0), TD(Î»), and SARSA
- **Function Approximation**: Linear and nonlinear approximation in large state spaces
- **Deep RL**: Deep Q-Networks (DQN), policy gradients, and advanced techniques like MuZero
- **Human Feedback and Real-World Considerations**: Policy shaping, ChatGPT and RLHF

Readings are based on *Reinforcement Learning: An Introduction* (Sutton & Barto, 2nd Edition), Chapters 1â€“11.

---

## ğŸ“ Assignment Summaries

### ğŸ§® Assignment 1 â€“ Dynamic Programming
Implemented policy evaluation and improvement using tabular value iteration and policy iteration. Focused on model-based planning in small, discrete MDPs.

### ğŸ¯ Assignment 2 â€“ Monte Carlo and Temporal-Difference Learning
Applied Monte Carlo and TD(0) methods to the CartPole environment. Compared learning performance and visualized episode rewards.

### ğŸ“‰ Assignment 3 â€“ Function Approximation
Extended TD learning with linear function approximation for continuous state spaces. Implemented batch updates and evaluated generalization.

### ğŸ¤– Assignment 4 â€“ Deep Q-Networks (DQN)
Built a Deep Q-Network (DQN) using PyTorch. Incorporated experience replay, target networks, and epsilon-greedy exploration in a high-dimensional setting.

---

## ğŸ“— Exercise Summaries

### ğŸ§­ Exercise 1 â€“ RL Foundations and MDPs
Introduced the agent-environment framework, MDPs, and core RL terminology. Explored return computation and value functions conceptually and programmatically.

### ğŸ•¹ï¸ Exercise 2 â€“ Policy Evaluation in CartPole
Implemented policy rollouts and return tracking in CartPole. Gained experience with OpenAI Gym environments and episode-based analysis.

### ğŸ”„ Exercise 3 â€“ Monte Carlo and TD in CartPole
Used Monte Carlo and TD(0) methods for value estimation. Compared learning stability, sample efficiency, and convergence characteristics.

### ğŸ§® Exercise 4 â€“ Value Function Approximation
Applied linear function approximation to generalize across continuous states. Built and analyzed value estimators using feature representations.

---

## âš ï¸ Academic Integrity

This repository is for **educational reference only**. If you are currently enrolled in APS1080, please do not copy or submit this work as your own.
