{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment 1**\n",
        "\n",
        "Parsa Youssefpour\n"
      ],
      "metadata": {
        "id": "LYXZ6ci-l-Ht"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsGqubiDkQnd"
      },
      "source": [
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iS40R9okStg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a9b5cd9-54b5-428d-fad3-526a24ee6bd8"
      },
      "source": [
        "gym.envs.register(\n",
        "    id='FrozenLakeNotSlippery-v0',\n",
        "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
        "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
        "    max_episode_steps=100,\n",
        "    reward_threshold=0.74\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:542: UserWarning: \u001b[33mWARN: Overriding environment FrozenLakeNotSlippery-v0\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {spec.id}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVX1AjRWkueO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f3da84-6d2b-4234-afd0-a21485857da8"
      },
      "source": [
        "# Create the gridworld-like environment\n",
        "env=gym.make('FrozenLakeNotSlippery-v0', render_mode='human')\n",
        "# Let's look at the model of the environment (i.e., P):\n",
        "env.env.P\n",
        "# Question: what is the data in this structure saying? Relate this to the course\n",
        "# presentation of P\n",
        "'''\n",
        "ANSWER:\n",
        "The printed model shows a dictionary of the 16 states of the game. In each state there are four moves that\n",
        "can be taken. Left, Down, Right, and Up, which are represented by numbers 0,1,2, and 3 respectively. The key\n",
        "value for each move in a state shows the porbability of reaching the next state if that move is taken, the next\n",
        "state, the expected reward from transition in to that state (1 for reaching the goal, and zero for other cases),\n",
        "and boolean value to determine whether the game has ended or not respectively.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {0: [(1.0, 0, 0.0, False)],\n",
              "  1: [(1.0, 4, 0.0, False)],\n",
              "  2: [(1.0, 1, 0.0, False)],\n",
              "  3: [(1.0, 0, 0.0, False)]},\n",
              " 1: {0: [(1.0, 0, 0.0, False)],\n",
              "  1: [(1.0, 5, 0.0, True)],\n",
              "  2: [(1.0, 2, 0.0, False)],\n",
              "  3: [(1.0, 1, 0.0, False)]},\n",
              " 2: {0: [(1.0, 1, 0.0, False)],\n",
              "  1: [(1.0, 6, 0.0, False)],\n",
              "  2: [(1.0, 3, 0.0, False)],\n",
              "  3: [(1.0, 2, 0.0, False)]},\n",
              " 3: {0: [(1.0, 2, 0.0, False)],\n",
              "  1: [(1.0, 7, 0.0, True)],\n",
              "  2: [(1.0, 3, 0.0, False)],\n",
              "  3: [(1.0, 3, 0.0, False)]},\n",
              " 4: {0: [(1.0, 4, 0.0, False)],\n",
              "  1: [(1.0, 8, 0.0, False)],\n",
              "  2: [(1.0, 5, 0.0, True)],\n",
              "  3: [(1.0, 0, 0.0, False)]},\n",
              " 5: {0: [(1.0, 5, 0, True)],\n",
              "  1: [(1.0, 5, 0, True)],\n",
              "  2: [(1.0, 5, 0, True)],\n",
              "  3: [(1.0, 5, 0, True)]},\n",
              " 6: {0: [(1.0, 5, 0.0, True)],\n",
              "  1: [(1.0, 10, 0.0, False)],\n",
              "  2: [(1.0, 7, 0.0, True)],\n",
              "  3: [(1.0, 2, 0.0, False)]},\n",
              " 7: {0: [(1.0, 7, 0, True)],\n",
              "  1: [(1.0, 7, 0, True)],\n",
              "  2: [(1.0, 7, 0, True)],\n",
              "  3: [(1.0, 7, 0, True)]},\n",
              " 8: {0: [(1.0, 8, 0.0, False)],\n",
              "  1: [(1.0, 12, 0.0, True)],\n",
              "  2: [(1.0, 9, 0.0, False)],\n",
              "  3: [(1.0, 4, 0.0, False)]},\n",
              " 9: {0: [(1.0, 8, 0.0, False)],\n",
              "  1: [(1.0, 13, 0.0, False)],\n",
              "  2: [(1.0, 10, 0.0, False)],\n",
              "  3: [(1.0, 5, 0.0, True)]},\n",
              " 10: {0: [(1.0, 9, 0.0, False)],\n",
              "  1: [(1.0, 14, 0.0, False)],\n",
              "  2: [(1.0, 11, 0.0, True)],\n",
              "  3: [(1.0, 6, 0.0, False)]},\n",
              " 11: {0: [(1.0, 11, 0, True)],\n",
              "  1: [(1.0, 11, 0, True)],\n",
              "  2: [(1.0, 11, 0, True)],\n",
              "  3: [(1.0, 11, 0, True)]},\n",
              " 12: {0: [(1.0, 12, 0, True)],\n",
              "  1: [(1.0, 12, 0, True)],\n",
              "  2: [(1.0, 12, 0, True)],\n",
              "  3: [(1.0, 12, 0, True)]},\n",
              " 13: {0: [(1.0, 12, 0.0, True)],\n",
              "  1: [(1.0, 13, 0.0, False)],\n",
              "  2: [(1.0, 14, 0.0, False)],\n",
              "  3: [(1.0, 9, 0.0, False)]},\n",
              " 14: {0: [(1.0, 13, 0.0, False)],\n",
              "  1: [(1.0, 14, 0.0, False)],\n",
              "  2: [(1.0, 15, 1.0, True)],\n",
              "  3: [(1.0, 10, 0.0, False)]},\n",
              " 15: {0: [(1.0, 15, 0, True)],\n",
              "  1: [(1.0, 15, 0, True)],\n",
              "  2: [(1.0, 15, 0, True)],\n",
              "  3: [(1.0, 15, 0, True)]}}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyn_w3ulkyZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f91fde91-d7a3-48a2-8341-a8cc000d3845"
      },
      "source": [
        "# Now let's investigate the observation space (i.e., S using our nomenclature),\n",
        "# and confirm we see it is a discrete space with 16 locations\n",
        "print(env.observation_space)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zND5ArI8k_qQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bfc156e-3370-453a-ac42-4a005beef00b"
      },
      "source": [
        "stateSpaceSize = env.observation_space.n\n",
        "print(stateSpaceSize)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_tp9YzRljnj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22dd1ad7-35cf-4605-b7fa-5cfc77502516"
      },
      "source": [
        "# Now let's investigate the action space (i.e., A) for the agent->environment\n",
        "# channel\n",
        "print(env.action_space)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFGNZNowluz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3471f88-ccd1-40f9-fd65-f41e180358af"
      },
      "source": [
        "# The gym environment has ...sample() functions that allow us to sample\n",
        "# from the above spaces:\n",
        "for g in range(1,10,1):\n",
        "  print(\"sample from S:\",env.observation_space.sample(),\" ... \",\"sample from A:\",env.action_space.sample())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample from S: 9  ...  sample from A: 3\n",
            "sample from S: 7  ...  sample from A: 3\n",
            "sample from S: 11  ...  sample from A: 3\n",
            "sample from S: 15  ...  sample from A: 1\n",
            "sample from S: 7  ...  sample from A: 2\n",
            "sample from S: 11  ...  sample from A: 3\n",
            "sample from S: 6  ...  sample from A: 1\n",
            "sample from S: 3  ...  sample from A: 0\n",
            "sample from S: 9  ...  sample from A: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOQL5JxsmcEd"
      },
      "source": [
        "# The enviroment also provides a helper to render (visualize) the environment\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLV6e43mmwx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5eef8ad-b346-43c7-ccc4-36f34b8ad385"
      },
      "source": [
        "# We can act as the agent, by selecting actions and stepping the environment\n",
        "# through time to see its responses to our actions\n",
        "env.reset()\n",
        "exitCommand=False\n",
        "while not(exitCommand):\n",
        "  env.render()\n",
        "  print(\"Enter the action as an integer from 0 to\",env.action_space.n,\" (or exit): \")\n",
        "  userInput=input()\n",
        "  if userInput==\"exit\":\n",
        "    break\n",
        "  action=int(userInput)\n",
        "  (observation, reward, compute, probability) = env.step(action)\n",
        "  print(\"--> The result of taking action\",action,\"is:\")\n",
        "  print(\"     S=\",observation)\n",
        "  print(\"     R=\",reward)\n",
        "  print(\"     p=\",probability)\n",
        "\n",
        "  env.render()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the action as an integer from 0 to 4  (or exit): \n",
            "1\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Enter the action as an integer from 0 to 4  (or exit): \n",
            "1\n",
            "--> The result of taking action 1 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Enter the action as an integer from 0 to 4  (or exit): \n",
            "1\n",
            "--> The result of taking action 1 is:\n",
            "     S= 12\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Enter the action as an integer from 0 to 4  (or exit): \n",
            "2\n",
            "--> The result of taking action 2 is:\n",
            "     S= 12\n",
            "     R= 0\n",
            "     p= {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Enter the action as an integer from 0 to 4  (or exit): \n",
            "0\n",
            "--> The result of taking action 0 is:\n",
            "     S= 12\n",
            "     R= 0\n",
            "     p= {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Enter the action as an integer from 0 to 4  (or exit): \n",
            "exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "35jqOWn7o-C0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tBpeiuRnyih"
      },
      "source": [
        "# Question: draw a table indicating the correspondence between the action\n",
        "# you input (a number) and the logic action performed.\n",
        "'''\n",
        "ANSWER:\n",
        "INPUT : LOGIC ACTION PERFORMED\n",
        "0     : The player moved LEFT\n",
        "1     : The player moved DOWN\n",
        "2     : The player moved RIGHT\n",
        "3     : The player moved UP\n",
        "'''\n",
        "# Question: draw a table that illustrates what the symbols on the render image\n",
        "# mean?\n",
        "'''\n",
        "ANSWER:\n",
        "SYMBOLS   : MEANING\n",
        "S         : Starting Point\n",
        "F         : Frozen Surface (Safe)\n",
        "H         : Lake (Unsafe)/ results in a loss\n",
        "G         : Goal - the position we intend to reach to win the game\n",
        "'''\n",
        "# Question: Explain what the objective of the agent is in this environment?\n",
        "\n",
        "'''\n",
        "ANSWER:\n",
        "The Objective of the game is to go from the staring position (S) to the goal\n",
        "position(G), by moving Up, Down, Left, or Right, without passing through H\n",
        "which is the lake in this example.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "yah-jRTDo8Gw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWI3h6s7qqdq"
      },
      "source": [
        "# Practical: Code up an AI that will employ random action selection in order\n",
        "# to drive the agent. Test this random action selection agent with the\n",
        "# above environment (i.e., code up a loop as I did above, but instead\n",
        "# of taking input from a human user, take it from the AI you coded)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "Ewj5XiukBKnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmRwGwPoqw0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "957c8e73-fbdd-46fd-ce92-34840fdc91d2"
      },
      "source": [
        "env.reset()\n",
        "exitCommand=False\n",
        "while not(exitCommand):\n",
        "  env.render()\n",
        "  action=random.choice([0,1,2,3])\n",
        "  moves = {0:\"LEFT\",1:\"DOWN\",2:\"RIGHT\",3:\"Up\"}                      #dictionary to print the action taken\n",
        "  print(f\"Action Taken {moves[action]}\")\n",
        "  (observation, reward, exitCommand, probability) = env.step(action)\n",
        "  print(\"--> The result of taking action\",action,\"is:\")\n",
        "  print(\"     S=\",observation)\n",
        "  print(\"     R=\",reward)\n",
        "  print(\"     p=\",probability)\n",
        "\n",
        "  env.render()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Taken RIGHT\n",
            "--> The result of taking action 2 is:\n",
            "     S= 1\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken LEFT\n",
            "--> The result of taking action 0 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken DOWN\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken Up\n",
            "--> The result of taking action 3 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken Up\n",
            "--> The result of taking action 3 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken DOWN\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken Up\n",
            "--> The result of taking action 3 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken LEFT\n",
            "--> The result of taking action 0 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken RIGHT\n",
            "--> The result of taking action 2 is:\n",
            "     S= 1\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken DOWN\n",
            "--> The result of taking action 1 is:\n",
            "     S= 5\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0, 'TimeLimit.truncated': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "YFc6KiHpo40q"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQLKuubzrl4L"
      },
      "source": [
        "# Now towards dynamic programming. Note that env.env.P has the model\n",
        "# of the environment.\n",
        "#\n",
        "# Question: How would you represent the agent's policy function and value function?\n",
        "'''\n",
        "ANSWER:\n",
        "I will represent my policy via a dictionary with each state as the key and the value\n",
        "another dictionary with the 4 possible moves as the keys and the probability of the\n",
        "action being taken as the values.\n",
        "\n",
        "To represent the value function, I will create a library with each state as the key\n",
        "and the value of the dictionary will be that states value function.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "_8aZnoYEo25q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical: revise the above AI solver to use a policy function in which you\n",
        "# code the random action selections in the policy function. Test this.\n",
        "\n",
        "import numpy as np\n",
        "#creating a function that given the state of the game return the action with the highest probablity\n",
        "def policy_action(state):\n",
        "  #creating an empty dict for the policy\n",
        "\n",
        "  # Goinh through all the available states and generating a random probability for each action at each state, by creating a nested dict\n",
        "  for key in env.env.P:\n",
        "    prob = np.random.dirichlet(np.ones(4),size=1)                               #creating a list of 4 random values that add up to 1\n",
        "    rand_policy[key]= {0:prob[0][0], 1:prob[0][1], 2:prob[0][2], 3:prob[0][3]}\n",
        "  return max(rand_policy[state], key = rand_policy[state].get)\n"
      ],
      "metadata": {
        "id": "ciA1skhJKEhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TESTING THE POLICY\n",
        "env.reset()\n",
        "exitCommand=False\n",
        "state=0\n",
        "while not(exitCommand):                                             # end the loop once the game ends\n",
        "  env.render()\n",
        "  action=policy_action(state)                                       #finding the best action, this generates a randome policy every round\n",
        "  moves = {0:\"LEFT\",1:\"DOWN\",2:\"RIGHT\",3:\"Up\"}                      #dictionary to print the action taken\n",
        "  print(f\"Action Taken {moves[action]}\")\n",
        "  (state, reward, exitCommand, probability) = env.step(action)      #take action\n",
        "  print(\"--> The result of taking action\",action,\"is:\")\n",
        "  print(\"     S=\",state)\n",
        "  print(\"     R=\",reward)\n",
        "  print(\"     p=\",probability)\n",
        "\n",
        "  env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHcaZCFrP94q",
        "outputId": "a717947f-d558-4950-f79f-2d27e94c37d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Taken DOWN\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken DOWN\n",
            "--> The result of taking action 1 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken LEFT\n",
            "--> The result of taking action 0 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken RIGHT\n",
            "--> The result of taking action 2 is:\n",
            "     S= 9\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken DOWN\n",
            "--> The result of taking action 1 is:\n",
            "     S= 13\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken RIGHT\n",
            "--> The result of taking action 2 is:\n",
            "     S= 14\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken Up\n",
            "--> The result of taking action 3 is:\n",
            "     S= 10\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken LEFT\n",
            "--> The result of taking action 0 is:\n",
            "     S= 9\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken LEFT\n",
            "--> The result of taking action 0 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken LEFT\n",
            "--> The result of taking action 0 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken Up\n",
            "--> The result of taking action 3 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken DOWN\n",
            "--> The result of taking action 1 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken RIGHT\n",
            "--> The result of taking action 2 is:\n",
            "     S= 9\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken RIGHT\n",
            "--> The result of taking action 2 is:\n",
            "     S= 10\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken DOWN\n",
            "--> The result of taking action 1 is:\n",
            "     S= 14\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken LEFT\n",
            "--> The result of taking action 0 is:\n",
            "     S= 13\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken LEFT\n",
            "--> The result of taking action 0 is:\n",
            "     S= 12\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0, 'TimeLimit.truncated': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "g3qPi1tpo0HY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical: Code the C-4 Policy Evaluation (Prediction) algorithm. You may use\n",
        "# either the inplace or ping-pong buffer (as described in the lecture). Now\n",
        "# randomly initialize your policy function, and compute its value function.\n",
        "# Report your results: policy and value function. Ensure your prediction\n",
        "# algo reports how many iterations it took.\n",
        "\n",
        "\n",
        "#creating an empty dict for the policy\n",
        "rand_policy={}\n",
        "for key in env.env.P:\n",
        "  prob = np.random.dirichlet(np.ones(4),size=1)                                 # creating a list of 4 random values between 0 and 1 that add up to 1\n",
        "  rand_policy[key]= {0:prob[0][0], 1:prob[0][1], 2:prob[0][2], 3:prob[0][3]}    # assinging a random value for each action"
      ],
      "metadata": {
        "id": "P5eC0NqzPEg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_eval(policy, gamma, theta):\n",
        "  #intializing the value function dic with 0s\n",
        "  value_fcn={}\n",
        "  for state in env.env.P:\n",
        "    value_fcn[state] = 0\n",
        "\n",
        "  #initializing the delta value and the number of iterations\n",
        "  delta = 2*theta\n",
        "  num_iteration = 0\n",
        "\n",
        "  #Policy Evaluation\n",
        "  while delta > theta:\n",
        "    delta = 0\n",
        "\n",
        "    for state in env.env.P:\n",
        "      #looping through all the states in the model\n",
        "\n",
        "      v_new = 0\n",
        "\n",
        "      for action in range(4):\n",
        "        # loop through from each action (UP, DOWN, LEFT, RIGHT) represented via 0,1,2, and 3\n",
        "\n",
        "        prob, next_state, reward, done = env.env.P[state][action][0]\n",
        "        #if the game is done then there is no next state, and therefore not considered\n",
        "        if done:\n",
        "          v_new += policy[state][action] * prob * reward\n",
        "        else:\n",
        "          v_new += policy[state][action] * prob * (reward+gamma*value_fcn[next_state])\n",
        "\n",
        "      delta =max(delta,abs(v_new-value_fcn[state]))\n",
        "      value_fcn[state] = v_new\n",
        "    num_iteration+=1\n",
        "\n",
        "  return value_fcn, num_iteration\n"
      ],
      "metadata": {
        "id": "lhW0UWEMPID1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values, num_iter = policy_eval(rand_policy, gamma = 0.9, theta = 0.0001)\n",
        "print(\"Value function:\")\n",
        "[print(key,':',round(value,4)) for key, value in values.items()]\n",
        "print(\"Number of Iterations:\", num_iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2tWFQN9T4DA",
        "outputId": "0e97edcb-7651-42dd-b14d-372ae13ae3b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value function:\n",
            "0 : 0.0014\n",
            "1 : 0.0024\n",
            "2 : 0.0154\n",
            "3 : 0.0106\n",
            "4 : 0.0058\n",
            "5 : 0.0\n",
            "6 : 0.057\n",
            "7 : 0.0\n",
            "8 : 0.0148\n",
            "9 : 0.1024\n",
            "10 : 0.1331\n",
            "11 : 0.0\n",
            "12 : 0.0\n",
            "13 : 0.2552\n",
            "14 : 0.3465\n",
            "15 : 0.0\n",
            "Number of Iterations: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "xAOSsE7noxbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional): Repeat the above for q."
      ],
      "metadata": {
        "id": "YYkQIZZpoMRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "TXsmt0l0pCoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy Improvement:\n",
        "# Question: How would you use P and your value function to improve an arbitrary\n",
        "# policy, pi, per Chapter 4?\n",
        "\n",
        "'''\n",
        "ANSWER:\n",
        "We can use methods such as policy iteration or value iteration to evaluate and\n",
        "improve the policy\n",
        "'''"
      ],
      "metadata": {
        "id": "j049kIQaKzKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "yC9rI1sNpEcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Practical: Code the policy iteration process, and employ it to arrive at a\n",
        "# policy that solves this problem. Show your testing results, and ensure\n",
        "# it reports the number of iterations for each step: (a) overall policy\n",
        "# iteration steps and (b) evaluation steps."
      ],
      "metadata": {
        "id": "k6VUFp4FnF8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improv(policy, gamma, value_fcn):\n",
        "\n",
        "  policy_stable = True                                                          # Initializing policy_stable\n",
        "\n",
        "  for state in env.env.P:\n",
        "    old_action = max(policy[state], key = policy[state].get)                    # argmax for dict, return the key with highest value\n",
        "    new_probs  = [0, 0, 0, 0]                                                   # Initialzing a list to store the new probabilites for each action\n",
        "\n",
        "    for action in range(4):\n",
        "\n",
        "      #calculating the new probailities for each action\n",
        "      prob, next_state, reward, done = env.env.P[state][action][0]\n",
        "\n",
        "      if done:                                                                  # If the game is over then there is no value function for the next state\n",
        "        new_probs[action] += prob * reward\n",
        "\n",
        "      else:\n",
        "        new_probs[action] += prob * (reward+gamma*value_fcn[next_state])\n",
        "\n",
        "    # taking argmax of the calculated probabities and updating the policy\n",
        "    for i in range(len(new_probs)):\n",
        "      if i == new_probs.index(max(new_probs)):\n",
        "        policy[state][i] = 1.0\n",
        "      else:\n",
        "        policy[state][i] = 0.0\n",
        "\n",
        "    # checking if the policy is stable for all states\n",
        "    if old_action != new_probs.index(max(new_probs)):\n",
        "      policy_stable = False\n",
        "\n",
        "  return policy, policy_stable\n"
      ],
      "metadata": {
        "id": "MMJR4cNuxH-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iter(gamma, theta):\n",
        "\n",
        "  # initializing policy dict\n",
        "  policy={}\n",
        "  for key in env.env.P:\n",
        "    policy[key]= {0:0.25, 1:0.25, 2:0.25, 3:0.25}    # Assigning the same prob for all actions\n",
        "\n",
        "  # value function is initialized int the policy_eval function\n",
        "\n",
        "  policy_stable =False\n",
        "\n",
        "  num_iter = 0\n",
        "\n",
        "  # Policy Interation\n",
        "  while not policy_stable:\n",
        "    value_fcn,v_iter = policy_eval(policy, gamma, theta)\n",
        "    print(f\"# Overall Iteration: {num_iter+1}, # Policy Evaluation Iteration: {v_iter}\")\n",
        "    print(f\"Value function {num_iter+1}\")\n",
        "    [print(key,'     :',round(value,4)) for key, value in value_fcn.items()]\n",
        "    print(\"\")\n",
        "    policy, policy_stable = policy_improv(policy, gamma, value_fcn)\n",
        "\n",
        "    num_iter +=1\n",
        "  return value_fcn, policy, num_iter\n"
      ],
      "metadata": {
        "id": "SA6FdFR-5KRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value_fcn, policy, num_iter = policy_iter(gamma = 0.9, theta =0.0001)\n",
        "\n",
        "print(\"---------------------\")\n",
        "print(\"FINAL Value Function\")\n",
        "print(\"STATE  :  VALUE\")\n",
        "[print(key,'     :',round(value,4)) for key, value in value_fcn.items()]\n",
        "print(\"\")\n",
        "\n",
        "print(\"Policy\")\n",
        "print(\"State :              Policy\")\n",
        "[print(key,'    :',value) for key, value in policy.items()]\n",
        "print(\"\")\n",
        "\n",
        "print(\"Number of Iterations: \", num_iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTeHqBc26Ykb",
        "outputId": "2413d7f4-70ae-4b4e-cd19-195bf3099dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Overall Iteration: 1, # Policy Evaluation Iteration: 15\n",
            "Value function 1\n",
            "0      : 0.0043\n",
            "1      : 0.0041\n",
            "2      : 0.01\n",
            "3      : 0.0041\n",
            "4      : 0.0066\n",
            "5      : 0.0\n",
            "6      : 0.0263\n",
            "7      : 0.0\n",
            "8      : 0.0186\n",
            "9      : 0.0576\n",
            "10      : 0.107\n",
            "11      : 0.0\n",
            "12      : 0.0\n",
            "13      : 0.1304\n",
            "14      : 0.3915\n",
            "15      : 0.0\n",
            "\n",
            "# Overall Iteration: 2, # Policy Evaluation Iteration: 7\n",
            "Value function 2\n",
            "0      : 0.5905\n",
            "1      : 0.6561\n",
            "2      : 0.729\n",
            "3      : 0.6561\n",
            "4      : 0.6561\n",
            "5      : 0.0\n",
            "6      : 0.81\n",
            "7      : 0.0\n",
            "8      : 0.729\n",
            "9      : 0.81\n",
            "10      : 0.9\n",
            "11      : 0.0\n",
            "12      : 0.0\n",
            "13      : 0.9\n",
            "14      : 1.0\n",
            "15      : 0.0\n",
            "\n",
            "---------------------\n",
            "FINAL Value Function\n",
            "STATE  :  VALUE\n",
            "0      : 0.5905\n",
            "1      : 0.6561\n",
            "2      : 0.729\n",
            "3      : 0.6561\n",
            "4      : 0.6561\n",
            "5      : 0.0\n",
            "6      : 0.81\n",
            "7      : 0.0\n",
            "8      : 0.729\n",
            "9      : 0.81\n",
            "10      : 0.9\n",
            "11      : 0.0\n",
            "12      : 0.0\n",
            "13      : 0.9\n",
            "14      : 1.0\n",
            "15      : 0.0\n",
            "\n",
            "Policy\n",
            "State :              Policy\n",
            "0     : {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0}\n",
            "1     : {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0}\n",
            "2     : {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0}\n",
            "3     : {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
            "4     : {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0}\n",
            "5     : {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
            "6     : {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0}\n",
            "7     : {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
            "8     : {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0}\n",
            "9     : {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0}\n",
            "10     : {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0}\n",
            "11     : {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
            "12     : {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
            "13     : {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0}\n",
            "14     : {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0}\n",
            "15     : {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
            "\n",
            "Number of Iterations:  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### TESTING THE POLICY ###\n",
        "env.reset()\n",
        "exitCommand=False\n",
        "state=0\n",
        "while not(exitCommand):                                             # end the loop once the game ends\n",
        "  env.render()\n",
        "  action=max(policy[state], key = policy[state].get)                #finding the best action\n",
        "  moves = {0:\"LEFT\",1:\"DOWN\",2:\"RIGHT\",3:\"Up\"}                      #dictionary to print the action taken\n",
        "  print(f\"Action Taken {moves[action]}\")\n",
        "  (state, reward, exitCommand, probability) = env.step(action)      #take action\n",
        "  print(\"--> The result of taking action\",action,\"is:\")\n",
        "  print(\"     S=\",state)\n",
        "  print(\"     R=\",reward)\n",
        "  print(\"     p=\",probability)\n",
        "\n",
        "  env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DehjZPDOYrp5",
        "outputId": "4e884fb9-7c5c-433d-e902-494dc3544a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Taken DOWN\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken DOWN\n",
            "--> The result of taking action 1 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken RIGHT\n",
            "--> The result of taking action 2 is:\n",
            "     S= 9\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken DOWN\n",
            "--> The result of taking action 1 is:\n",
            "     S= 13\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken RIGHT\n",
            "--> The result of taking action 2 is:\n",
            "     S= 14\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken RIGHT\n",
            "--> The result of taking action 2 is:\n",
            "     S= 15\n",
            "     R= 1.0\n",
            "     p= {'prob': 1.0, 'TimeLimit.truncated': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analysis:**\n",
        "\n",
        "Achieving a reward of 1 means tha the policy iteration was successful in creating a policy that resulted in maximizing the reward"
      ],
      "metadata": {
        "id": "6DdfiHtuZXPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical: Code the value iteration process, and employ it to arrive at a\n",
        "# policy that solves this problem. Show your testing results, reporting\n",
        "# the iteration counts."
      ],
      "metadata": {
        "id": "XRLSs4ccoKXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def value_eval(theta, gamma):\n",
        "  #intializing the value function dic with 0s\n",
        "  value_fcn={}\n",
        "  for state in env.env.P:\n",
        "    value_fcn[state] = 0\n",
        "\n",
        "  delta = 2*theta\n",
        "  num_iter = 0\n",
        "  while delta>theta:\n",
        "    delta = 0\n",
        "\n",
        "    for state in env.env.P:\n",
        "      v_new = [0, 0 ,0 ,0]                                                      # initializing a list to store the value for each action\n",
        "\n",
        "      for action in range(4):\n",
        "        prob, next_state, reward, done = env.env.P[state][action][0]\n",
        "\n",
        "        if done:\n",
        "          v_new[action] += prob * reward\n",
        "\n",
        "        else:\n",
        "          v_new[action] += prob * (reward + gamma*value_fcn[next_state])\n",
        "\n",
        "\n",
        "      v_max = max(v_new)                                                        #find the max value between the for action\n",
        "      delta = max(delta, abs(v_max-value_fcn[state]))                           #updating delta\n",
        "      value_fcn[state] = v_max                                                  #updating value function dict\n",
        "    num_iter+=1\n",
        "\n",
        "  policy={}\n",
        "  for key in env.env.P:\n",
        "    policy[key]= {0:0.25, 1:0.25, 2:0.25, 3:0.25}                               # Assigning the same prob for all actions\n",
        "\n",
        "  for state in env.env.P:\n",
        "    new_probs  = [0, 0, 0, 0]                                                   # Initialzing a list to store the new probabilites for each action\n",
        "\n",
        "    for action in range(4):\n",
        "\n",
        "      #calculating the new probailities for each action\n",
        "      prob, next_state, reward, done = env.env.P[state][action][0]\n",
        "\n",
        "      if done:                                                                  # If the game is over then there is no value function for the next state\n",
        "        new_probs[action] += prob * reward\n",
        "\n",
        "      else:\n",
        "        new_probs[action] += prob * (reward+gamma*value_fcn[next_state])\n",
        "\n",
        "    # taking argmax of the calculated probabities and updating the policy\n",
        "    for i in range(len(new_probs)):\n",
        "      if i == new_probs.index(max(new_probs)):\n",
        "        policy[state][i] = 1.0\n",
        "      else:\n",
        "        policy[state][i] = 0.0\n",
        "\n",
        "  return value_fcn, policy, num_iter\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGRCfukr6slT",
        "outputId": "b049c13e-07b6-49c2-e703-97632a9f21d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "values, v_policy, iter = value_eval(theta=0.0001, gamma=0.9)\n",
        "\n",
        "print(\"---------------------\")\n",
        "print(\"FINAL Value Function\")\n",
        "print(\"STATE  :  VALUE\")\n",
        "[print(key,'     :',round(value,4)) for key, value in values.items()]\n",
        "print(\"\")\n",
        "\n",
        "print(\"Policy\")\n",
        "print(\"State :              Policy\")\n",
        "[print(key,'    :',value) for key, value in v_policy.items()]\n",
        "print(\"\")\n",
        "\n",
        "print(\"Number of Iterations: \", iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPs7PxmLH7rJ",
        "outputId": "3e4bfe26-5796-45f9-b06e-941a95124814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "FINAL Value Function\n",
            "STATE  :  VALUE\n",
            "0      : 0.5905\n",
            "1      : 0.6561\n",
            "2      : 0.729\n",
            "3      : 0.6561\n",
            "4      : 0.6561\n",
            "5      : 0.0\n",
            "6      : 0.81\n",
            "7      : 0.0\n",
            "8      : 0.729\n",
            "9      : 0.81\n",
            "10      : 0.9\n",
            "11      : 0.0\n",
            "12      : 0.0\n",
            "13      : 0.9\n",
            "14      : 1.0\n",
            "15      : 0.0\n",
            "\n",
            "Policy\n",
            "State :              Policy\n",
            "0     : {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0}\n",
            "1     : {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0}\n",
            "2     : {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0}\n",
            "3     : {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
            "4     : {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0}\n",
            "5     : {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
            "6     : {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0}\n",
            "7     : {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
            "8     : {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0}\n",
            "9     : {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0}\n",
            "10     : {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0}\n",
            "11     : {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
            "12     : {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
            "13     : {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0}\n",
            "14     : {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0}\n",
            "15     : {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
            "\n",
            "Number of Iterations:  7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### TESTING THE POLICY\n",
        "env.reset()\n",
        "exitCommand=False\n",
        "state=0\n",
        "while not(exitCommand):                                             # end the loop once the game ends\n",
        "  env.render()\n",
        "  action=max(v_policy[state], key = v_policy[state].get)            #finding the best action\n",
        "  moves = {0:\"LEFT\",1:\"DOWN\",2:\"RIGHT\",3:\"Up\"}                      #dictionary to print the action taken\n",
        "  print(f\"Action Taken {moves[action]}\")\n",
        "  (state, reward, exitCommand, probability) = env.step(action)      #take action\n",
        "  print(\"--> The result of taking action\",action,\"is:\")\n",
        "  print(\"     S=\",state)\n",
        "  print(\"     R=\",reward)\n",
        "  print(\"     p=\",probability)\n",
        "\n",
        "  env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lupGMz_UaF54",
        "outputId": "bef0dec4-7a4c-4a37-8401-3cce3be6dd82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Taken DOWN\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken DOWN\n",
            "--> The result of taking action 1 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken RIGHT\n",
            "--> The result of taking action 2 is:\n",
            "     S= 9\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken DOWN\n",
            "--> The result of taking action 1 is:\n",
            "     S= 13\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken RIGHT\n",
            "--> The result of taking action 2 is:\n",
            "     S= 14\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Action Taken RIGHT\n",
            "--> The result of taking action 2 is:\n",
            "     S= 15\n",
            "     R= 1.0\n",
            "     p= {'prob': 1.0, 'TimeLimit.truncated': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analysis:**\n",
        "\n",
        "Achieving a reward of 1 means that the policy achieved is correct"
      ],
      "metadata": {
        "id": "vKjsenfoarVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comment on the difference between the iterations required for policy vs\n",
        "# value iteration.\n",
        "\n",
        "'''\n",
        "It can be seen that with both the policy iteration and value iteration\n",
        "we were able to reach a policy that was able to win the game by reaching\n",
        "the goal position without falling through a hole.\n",
        "\n",
        "When looking at the number of iterations, it can be seen that the policy\n",
        "iteration was able to reach the policy within 2 iterations, while it took\n",
        "value iteration 7 iterations. This would indicate that policy iteration is\n",
        "more efficient. Upon closer inspection, it can be seen that in the policy\n",
        "evaluation of the policy iteration method, goes through 17 and 5 iterations\n",
        "respectively over the 2 overall interations. This would indicate that value\n",
        "iteration policy maybe more efficient.\n",
        "'''"
      ],
      "metadata": {
        "id": "YIYN3hKJ6c2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: instead of the above environment, use the \"slippery\" Frozen Lake via\n",
        "# env = gym.make(\"FrozenLake-v0\")"
      ],
      "metadata": {
        "id": "cUVN_l_S6VK9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}